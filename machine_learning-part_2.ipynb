{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4db435",
   "metadata": {},
   "source": [
    "# Machine Learning - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3800f",
   "metadata": {},
   "source": [
    "## Machine learning paradigms\n",
    "\n",
    "There are many different types of models in machine learning and choosing the best one is dependent on:\n",
    "1. The problem you aim to solve\n",
    "2. The data you have\n",
    "\n",
    "In some instances multiple models may work well for you, in which case you will have to consider other aspects of the model, such as:\n",
    "* interpretability\n",
    "* memory cost\n",
    "* number of samples\n",
    "* dimensionality\n",
    "* and so on...\n",
    "\n",
    "Though these considerations may help you narrow down your choices, choosing the *best* remains a difficult task. I will provide some general information about different types of machine learning models while keeping some of the above aspects in mind.\n",
    "\n",
    "Below is a figure that shows a very well defined hierarchy of different ML models that one can consider. The upper level of this hierarchy gives 3 main learning paradigms: **supervised**, **unsupervised**, and **reinforcement**. I will discuss all 3 of these as well as a fourth, called **semi-supervised**.\n",
    "\n",
    "<img width=\"500px\" src=\"img/ml_hierarchy.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "        roc_auc_score, auc, precision_recall_curve, roc_curve, log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import tree\n",
    "from IPython.display import Image \n",
    "\n",
    "import random\n",
    "## set seed for randomization\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896be6e6",
   "metadata": {},
   "source": [
    "## Pima Indians Diabetes dataset\n",
    "We will use the Pima Indians dataset to experiment with decision trees. The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive on a carbohydrate poor diet. In recent years, a sudden shift from traditional agricultural crops to processed foods and a decline in physical activity led to a high prevalence of type 2 diabetes in this population.  \n",
    "\n",
    "The dataset can be downloaded here:\n",
    "\n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database#diabetes.csv\n",
    "\n",
    "I have named the downloaded file: `diabetes.csv`.\n",
    "\n",
    "The dataset includes data from 768 women. The columns are defined as follows:\n",
    "\n",
    "* `Pregnancies`: Number of times pregnant\n",
    "* `Glucose`: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "* `BloodPressure`: Diastolic blood pressure (mm Hg)\n",
    "* `SkinThickness`: Triceps skin fold thickness (mm)\n",
    "* `Insulin`: 2-Hour serum insulin (mu U/ml)\n",
    "* `BMI`: Body mass index (weight in kg/(height in m)^2)\n",
    "* `DiabetesPedigreeFunction`: The output of the pedigree function that provides measure of genetic influence and gives us an idea of the hereditary risk one might have with the onset of diabetes mellitus\n",
    "* `Age`: Age (years)\n",
    "* `Outcome`: Class variable (0 or 1) 268 of 768 are 1 (positive), the others are 0 (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Pima Indians Diabetes dataset (downloaded May 14, 2019; N=768)\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd629011",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to determine if a row has an missing value\n",
    "def valid_value(row):\n",
    "    if 0 == row['Glucose'] or \\\n",
    "       0 == row['BloodPressure'] or \\\n",
    "       0 == row['SkinThickness'] or \\\n",
    "       0 == row['Insulin'] or \\\n",
    "       0 == row['BMI'] or \\\n",
    "       0 == row['Age']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "## create dataframe with only valid rows\n",
    "df_pima = df[df.apply(lambda row: valid_value(row), axis=1)]\n",
    "df_pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf692a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"length of original dataframe: {len(df)}\")\n",
    "print(f\"length of filtered dataframe: {len(df_pima)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb86918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset in features and target variable\n",
    "feature_cols = \\\n",
    "    ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose',\n",
    "     'BloodPressure','DiabetesPedigreeFunction', 'SkinThickness']\n",
    "\n",
    "X = df_pima[feature_cols]\n",
    "y = df_pima['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af305e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_pima['Outcome']) # 70% training and 30% test\n",
    "print(f\"Number of samples in trianing set = {len(X_train)}\")\n",
    "print(f\"Number of samples in testing set = {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25753ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of positive samples in training set = {y_train.to_list().count(1)}\")\n",
    "print(f\"Number of negative samples in training set = {y_train.to_list().count(0)}\")\n",
    "print(f\"Ratio of positive to negative samples in training set = {y_train.to_list().count(1)/y_train.to_list().count(0):.3f}\\n\")\n",
    "\n",
    "print(f\"Number of positive samples in testing set = {y_test.to_list().count(1)}\")\n",
    "print(f\"Number of negative samples in testing set = {y_test.to_list().count(0)}\")\n",
    "print(f\"Ratio of positive to negative samples in testing set = {y_test.to_list().count(1)/y_test.to_list().count(0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce45b1",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "Random forest classifers are similar to decision trees in that they use hierarchical structures to split the dataset based on features. However, unlike decision trees, these classifiers use muliple decision trees (a \"forest\") in classification process using a method called *bagging*. Random forest is called an *ensemble* method because we have multiple classifiers by which we make our final prediction.\n",
    "\n",
    "The random forest algorithm consists of four general steps:\n",
    "* Select random samples from a given dataset - *bootstrapping*.\n",
    "* Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "* Perform a vote for each predicted result.\n",
    "* Select the prediction result with the most votes as the final prediction - *aggregating*.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "  <img width=\"500px\" src=\"img/random_forest_voting.png\" />\n",
    "  <em><small>Image taken from <a href=\"https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/\">Bagging vs Boosting in Machine Learning</a></small></em>\n",
    "</p>\n",
    "\n",
    "\n",
    "**Advantages**\n",
    "* Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "* It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "* The algorithm can be used in both classification and regression problems.\n",
    "* Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "* You can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
    "\n",
    "**Disadvantages**\n",
    "* Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
    "* The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70d1dc",
   "metadata": {},
   "source": [
    "## Implementing random forest\n",
    "Like decision trees, building and fitting a random forest classifier is a straightforward task  in scikit-learn. First, we define a random forest classifier variable, and, second, we train the classifier by calling the `fit` method.\n",
    "\n",
    "Random forest has many hyperparameters. Hyperparameters included in Random Forest are:\n",
    "* `n_estimators` = number of trees in the forest\n",
    "* `criterion` = the criterion used to choose a split at each node (e.g. gini, entropy, mse, etc.)\n",
    "* `max_depth` = maximum length of the longest route in each tree\n",
    "* `min_samples_split` = minimum number of samples to split on at a node\n",
    "* `max_leaf_nodes` = maximum number of leaf nodes\n",
    "* `max_features` = maximum number of random features to test at each node\n",
    "* `max_samples` = size of bootstrapped dataset for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eaa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build and fit random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_depth=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47667d6",
   "metadata": {},
   "source": [
    "## Evaluating random forest\n",
    "We can evaluate the our random forest classifier by calculating the accuracy, recall, precision, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = rfc.predict(X_test)\n",
    "y_proba_forest = list(zip(*rfc.predict_proba(X_test)))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_test, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    colors = sns.color_palette(\"Blues\")\n",
    "    ax = sns.heatmap([[tp,fp],[fn,tn]], square=True, annot=True, fmt='d', \n",
    "                     cbar=False, cmap=colors, vmin=-1, annot_kws={\"size\":13}, linewidths=1.0)\n",
    "    # set labels on figure\n",
    "    ax.set_xticklabels(labels=[\"pos\",\"neg\"], fontsize=13)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_yticklabels(labels=[\"pos\",\"neg\"], fontsize= 13)\n",
    "    plt.xlabel(\"\\nactual value\", fontsize=15)\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    plt.ylabel(\"predicted value\\n\", fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get values for confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_forest).ravel()\n",
    "print(f\"True Negative = {tn}\\nFalse Positive = {fp}\\nFalse Negative = {fn}\\nTrue Positive = {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae16fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## show confusion matrix for random forest\n",
    "show_confusion_matrix(y_test, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ee725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall = {recall_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e00bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision = {precision_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 score = {f1_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3786cf",
   "metadata": {},
   "source": [
    "As before, we can display the `confusion_matrix` of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(fpr, tpr, alpha=.5, color='darkorange')\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUROC = {auc(fpr,tpr):.3f}')\n",
    "    # Add dashed line with a slope of 1\n",
    "    ax.plot([0,1], [0,1], color='black', linestyle='dotted', lw=2, \\\n",
    "            label=f'Random = 0.500')\n",
    "    ax.legend()\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pr_curve(recall, precision, random_aupr):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(recall, precision, alpha=.5, color='blue')\n",
    "    ax.plot(recall, precision, color='blue', lw=2, label=f'AUPR = {auc(recall, precision):.3f}')\n",
    "    # Add dashed line where random (or no skill) would be\n",
    "    ax.plot([1,0], [random_aupr,random_aupr], color='black', linestyle='dotted', \\\n",
    "            lw=2, label=f'Random = {random_aupr:.3f}')\n",
    "    ax.legend()\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa093d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_test, y_proba_forest)\n",
    "print(\"AUROC\")\n",
    "print(f\"Random forest: {auc(fpr_forest,tpr_forest):.3f}\")\n",
    "print(f\"Random (no skill) AUROC: {auc([0,1], [0,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_forest,tpr_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47534e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_forest, recall_forest, thresholds_forest = precision_recall_curve(y_test, y_proba_forest)\n",
    "\n",
    "print(\"AUPR\")\n",
    "print(f\"Random forest: {auc(recall_forest, precision_forest):.3f}\")\n",
    "\n",
    "positive_class = y_test.to_list().count(1)\n",
    "negative_class = y_test.to_list().count(0)\n",
    "random_control = positive_class/(positive_class+negative_class)\n",
    "\n",
    "print(f\"Random (no skill) AUPR = {auc([0,1], [random_control,random_control]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98079ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(recall_forest,precision_forest,random_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11885536",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Cross-validation is key to choosing the best possible hyperparameters. This involves splitting the training set into $k$ number of subsets where one subset is used as a validation set and the remaining $k-1$ are used for training. This is then completed over all possible sets of $k$ and the average of the metrics is used to assess the model with the given hyperparameters.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "  <img width=\"800px\" src=\"img/k_fold_cv.png\"/>\n",
    "    <em><small>Image taken from <a href=\"https://www.sharpsightlabs.com/blog/cross-validation-explained/\">Cross Validation, Explained</a></small></em>\n",
    "</p>\n",
    "\n",
    "To further this idea, we can use cross-validation in concert with a *grid search* which runs a model with variable hyperparameters that are defined by lists of values. This will \"check\" the metrics for each of this runs and average them. The optimal combination of hyperparameters will be outputted as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees to be used\n",
    "rfc_n_estimators = [int(x) for x in np.linspace(100, 400, 4)]\n",
    "# Maximum length in tree\n",
    "rfc_max_depth = [int(x) for x in np.linspace(2, 8, 4)]\n",
    "rfc_max_features = [2,3,4]\n",
    "\n",
    "rfc_grid = {'n_estimators': rfc_n_estimators,\n",
    "            'max_depth': rfc_max_depth,\n",
    "            'max_features': rfc_max_features}\n",
    "\n",
    "# Create the model to be tuned\n",
    "rfc_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the random search Random Forest\n",
    "rfc_grid = GridSearchCV(estimator=rfc_base, param_grid=rfc_grid, \n",
    "                                cv=5, scoring='f1', n_jobs=2)\n",
    "\n",
    "# Fit the random search model\n",
    "rfc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal parameters\n",
    "rfc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = rfc_grid.predict(X_test)\n",
    "y_proba_grid = list(zip(*rfc_grid.predict_proba(X_test)))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_grid):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aabfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall = {recall_score(y_test, y_pred_grid):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision = {precision_score(y_test, y_pred_grid):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bab01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 score = {f1_score(y_test, y_pred_grid):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_grid, tpr_grid, thresholds_grid = roc_curve(y_test, y_proba_grid)\n",
    "print(\"AUROC\")\n",
    "print(f\"best rf: {auc(fpr_grid,tpr_grid):.3f}\")\n",
    "print(f\"Random (no skill) AUROC: {auc([0,1], [0,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_grid,tpr_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f58777",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_grid, recall_grid, thresholds_grid = precision_recall_curve(y_test, y_proba_grid)\n",
    "\n",
    "print(\"AUPR\")\n",
    "print(f\"best rf: {auc(recall_grid, precision_grid):.3f}\")\n",
    "print(f\"Random (no skill) AUPR = {auc([0,1], [random_control,random_control]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5aa6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pr_curve(recall_grid,precision_grid,random_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2794fc",
   "metadata": {},
   "source": [
    "## Feature ranking\n",
    "In addition to evaluating the random forest classifier, it is sometimes helpful to see how important each of the features were in arriving at final predictions. If we notice that a feature is of little importance, we can eliminate it from our training dataset in order to gain efficiency.\n",
    "\n",
    "When building a random forest classifier, scikit-learn returns a variable named `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find important features\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0e342",
   "metadata": {},
   "source": [
    "The raw output is a little difficult to interpret. So, we will put the output in a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d7ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pd.Series(rfc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9773935",
   "metadata": {},
   "source": [
    "We can also visualize the feature importances using a seaborn barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc627470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b8f89",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "How does this differ from Random Forest? Random Forest uses bagging in order to train a final model. XGBoost works by a method called **boosting**, which is an iterative, sequential method that adds a new decision tree to the overall model at each step to minimize error from the previous trees. Each new tree is a *weak learner* that when all combined creates a strong learner that will accurately predict the outcome.\n",
    "\n",
    "<img width=\"500px\" src=\"img/xgboost_boosting.png\" />\n",
    "\n",
    "A problem with XGBoost is that it is highly sensitive to it's hyperparameters. If too many trees are added, it can be overfit. Moreover, the `learning rate` is crucial because the model will perform better if trained slowly, but the likelihood of many trees being created increases with a decreaed learning rate. FInding the right balance for the model is key to the robustness and generalizability of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351d78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build and fit XGBoost classifier\n",
    "xgc = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, \\\n",
    "                        alpha=0.01, max_depth=4, learning_rate=0.1, \\\n",
    "                        colsample_bytree=0.3, verbosity=0)\n",
    "xgc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_boost = xgc.predict(X_test)\n",
    "y_proba_boost = list(zip(*xgc.predict_proba(X_test)))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69663f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3116f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_boost):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall = {recall_score(y_test, y_pred_boost):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision = {precision_score(y_test, y_pred_boost):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80f195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"F1 score = {f1_score(y_test, y_pred_boost):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45137b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_boost, tpr_boost, thresholds_boost = roc_curve(y_test, y_proba_boost)\n",
    "print(\"AUROC\")\n",
    "print(f\"best rf: {auc(fpr_boost,tpr_boost):.3f}\")\n",
    "print(f\"Random (no skill) AUROC: {auc([0,1], [0,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb59df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_boost,tpr_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb664cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_boost, recall_boost, thresholds_boost = precision_recall_curve(y_test, y_proba_boost)\n",
    "\n",
    "print(\"AUPR\")\n",
    "print(f\"xgboost: {auc(recall_boost, precision_boost):.3f}\")\n",
    "print(f\"Random (no skill) AUPR = {auc([0,1], [random_control,random_control]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4381bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pr_curve(recall_boost,precision_boost,random_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7e437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_imp = \\\n",
    "    pd.Series(xgc.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "## visualize important features\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('\\nFeature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74fe77",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Logistic regression is a simple and commonly used machine learning algorithm for two-class classification. It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Logistic regression can be used to answer questions such as:\n",
    "* How does the probability of getting lung cancer (yes vs. no) change for every additional pound a person is overweight and for every pack of cigarettes smoked per day?\n",
    "* Do body weight, calorie intake, fat intake, and age have an influence on the probability of having a heart attack (yes vs. no)?\n",
    "\n",
    "*Logistic regression can also be used for multi-class predictions, but we will not cover that here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ff531",
   "metadata": {},
   "source": [
    "In general, logistic regression uses a linear combination of more than one feature value or explanatory variable as argument of the sigmoid function:\n",
    "\n",
    "$f(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "The corresponding output of the sigmoid function is a number between 0 and 1. \n",
    "\n",
    "<img width=\"300px\" src=\"img/sigmoid.png\"/>\n",
    "\n",
    "The middle value is considered as threshold to establish what belongs to the class 1 and to the class 0. In particular, an input producing an outcome greater than 0.5 is considered belonging to the class 1. Conversely, if the output is less than 0.5, then the corresponding input is classified as belonging to class 0.\n",
    "\n",
    "For our logistic regression model we use the logistic function:\n",
    "\n",
    "$f_{w,b}(x) = \\frac{1}{1+e^{-(wx+b)}}$\n",
    "\n",
    "The logistic function is our **activation function**. This is going to tell us when a sample is 0 or 1.\n",
    "\n",
    "To calculate the solution to this equation, i.e. obtain the best intercept and coefficients, we aim to maximize the **log likelihood** of the training data.\n",
    "\n",
    "$ \\ln L_{\\mathbf{w},b} = \\sum_{i=1}^{N}y_{i}\\ln f_{\\mathbf{w},b}(x_{i})+(1-{y_{i}}) \\ln (1-f_{\\textbf{w},b}(x_{i})$\n",
    "\n",
    "Though it may appear daunting, when you break it down, it isn't that bad. When $y_{i}=1$, the second part of the summation drops out (1-1=0), whereas when $y_{i}=0$ the first part of the equation drops out. \n",
    "\n",
    "Log likelihood is our **cost function**. Generally, minimizing functions is preferred over maximizing, so the negative of the function is commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8518e",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "The general point is there are many methods that have been developed and well tested for optimizing (maximizing or minimizing) a function. You start with a guess then you adjust the parameters over several iterations until you have converged to some point (number of iterations, tolerance, etc.).\n",
    "\n",
    "<img width=\"300px\" src=\"img/minimization.gif\"/>\n",
    "\n",
    "\n",
    "I do not go into specific optimizers here, for the sake of time. I do think I will update this notebook in the future to include a nice section on optimizers.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "https://scikit-learn.org/stable/modules/sgd.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Limited-memory_BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0ddba",
   "metadata": {},
   "source": [
    "For our implementation of logistic regression, we will use scikit-learn's LogisticRegression model:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Let's load the libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score, auc, precision_recall_curve, roc_curve,\\\n",
    "    classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9d4ec",
   "metadata": {},
   "source": [
    "Finally, the test and training data is fit to our model and we predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a logistic regression classifier and predict\n",
    "logreg = LogisticRegression(random_state=42, max_iter=500)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "y_proba_logreg = logreg.predict_proba(X_test)[:,1]\n",
    "print(f\"Our model converged after {logreg.n_iter_[0]} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show confustion matrix\n",
    "show_confusion_matrix(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_logreg):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall = {recall_score(y_test, y_pred_logreg):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c645da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision = {precision_score(y_test, y_pred_logreg):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cce69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 score = {f1_score(y_test, y_pred_logreg):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_logreg, tpr_logreg, thresholds_logreg = roc_curve(y_test, y_proba_logreg)\n",
    "print(\"AUROC\")\n",
    "print(f\"logreg: {auc(fpr_logreg,tpr_logreg):.3f}\")\n",
    "print(f\"Random (no skill) AUROC: {auc([0,1], [0,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54600f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_logreg,tpr_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5603245",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_logreg, recall_logreg, thresholds_logreg = precision_recall_curve(y_test, y_proba_logreg)\n",
    "\n",
    "print(\"AUPR\")\n",
    "print(f\"logreg: {auc(recall_logreg, precision_logreg):.3f}\")\n",
    "print(f\"Random (no skill) AUPR = {auc([0,1], [random_control,random_control]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8171f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pr_curve(recall_logreg,precision_logreg,random_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691f4d6",
   "metadata": {},
   "source": [
    "### Interpretability and feature importance\n",
    "\n",
    "We will analyze the model a bit more to understand what the model is doing and what features are most important to the classification.\n",
    "\n",
    "First, we will extract the coefficients from the model and match them with the corresponding name of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7fe76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = list(zip(feature_cols,logreg.coef_[0]))\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11236763",
   "metadata": {},
   "source": [
    "Next, we take the exponential of the coefficients to calculate the odds ratio for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3014f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "odds_ratio = [(x[0],math.exp(x[1])) for x in coefs]\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611e1d7",
   "metadata": {},
   "source": [
    "We see that of these features the most important appears to be Diabetes Pedigree Function. The odds ratio tells us that for every 1 unit increase in Diabetes Pedigree Function a patient is 2.26x more likely to experience the outcome (diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc8f4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(x[0],(x[1]-1)*100.0) for x in odds_ratio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab2ebb",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Now that we understand and have run a logistic regression model, let's go a bit \"deeper\". We can think of a neural network (NN) as a set of nested functions -- we call these layers. Each layer in our model takes input from the previous layer and outputs directly to the next layer, i.e. fully connected. \n",
    "\n",
    "We are going to create a 3 layer neural network with the previously used 8 variables as features and the \"Outcome\" as the label. \n",
    "\n",
    "The first layer of our NN will take in all 8 features as input, has a ReLU (rectified linear unit) activation function, and outputs 24 latent features (hidden). As opposed to the logistic function, discussed previously, ReLU sets the input to 0 if it is <0 or uses the input as is if >0.\n",
    "\n",
    "$f(x)=max(0,x)$\n",
    "\n",
    "The second layer of our NN will take in all 24 latent features from the previous layer as input, has a ReLU (rectified linear unit) activation function, and outputs 12 latent features.\n",
    "\n",
    "The third (and last) layer of our model is a sigmoid output layer that takes in the previous 12 latent features as input.\n",
    "\n",
    "The loss function we use for this model is binary cross entropy, which basically sums the log probabilty of a given sample being in the 0 class and the log probability of the sample being in the 1 class across all samples. This is essentially the same function as the log likelihood. We want to minimize this loss function.\n",
    "\n",
    "$ \\ln Loss = \\sum_{i=1}^{N}-(y_{i}\\ln f(x_{i})+(1-{y_{i}}) \\ln (1-f(x_{i}))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de489eaf",
   "metadata": {},
   "source": [
    "For our implementation of neural network, we will use keras's sequential model:\n",
    "* https://keras.io/guides/sequential_model/\n",
    "\n",
    "Let's load the libraries we will be using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# summarize history for loss\n",
    "def plot_fit(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=8, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "# fit the keras model on the dataset\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=300, batch_size=20, verbose=False)\n",
    "\n",
    "# make class predictions with the model\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = (y_proba > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe21a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show confustion matrix\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53eab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28671ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99a7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate AUROC\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "# Calculate AUPR\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3fa93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tabulate the results in a dataframe\n",
    "df_pr = pd.DataFrame(list(zip(recall,precision)), columns=['recall','precision'])\n",
    "# Plot the PR curve\n",
    "plot_pr_curve(recall, precision, random_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc5c76",
   "metadata": {},
   "source": [
    "<font color='red'>***Note***</font>: \n",
    "Interesting artifact of this plot, the rapid drop to the left of the plot is due to the way the curve is calculated. As you move right to left on the plot the threshold for determining if a prediction is postive becomes more stringent. So on the far left, there are very few positive class predictions (both true and false positives), so adding one more false or true positive (moving towards the right) can greatly affect the precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d221a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=8, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "# fit the keras model on the dataset\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=300, batch_size=20, verbose=False)\n",
    "\n",
    "# make class predictions with the model\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = (y_proba > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60592d9e",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "The below figure is an excellent example of underfitting, a good fit, and overfitting.\n",
    "\n",
    "<img width=\"800px\" src=\"img/model_fit.png\"/>\n",
    "\n",
    "One can also plot learning curves to determine a good fit during training. This plots loss vs. epoch.\n",
    "\n",
    "<img width=\"900px\" src=\"img/learning_curves.png\"/>\n",
    "\n",
    "There are methods for avoiding overfitting/overtraining, such as regularization, dropout, etc. You can learn more about these in many of the references provided.\n",
    "\n",
    "https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "*Choosing the right model(s), activation function(s), and **hyperparameters** are crucial for creating a robust and **generalizable** model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=8, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "# fit the keras model on the dataset\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=20, verbose=False)\n",
    "plot_fit(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b0cc0",
   "metadata": {},
   "source": [
    "## References and additional reading\n",
    "\n",
    "In this module, we covered the basics of implementing and evaluating a logistic regression classifier in scikit learns and a neural network using keras. \n",
    "\n",
    "* Burkov A. The Hundred-Page Machine Learning Book by Andriy Burkov. Expert Systems. 2019;5(2):132-50.\n",
    "* Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J. Scikit-learn: Machine learning in Python. the Journal of machine Learning research. 2011 Nov 1;12:2825-30.\n",
    "* Chollet F. Keras documentation. keras.io. 2015;33.\n",
    "* Goodfellow I, Bengio Y, Courville A. Deep learning.\n",
    "* Bishop C. Pattern Recognition and Machine Learning.\n",
    "* Friedman JH, Tibshirani R, Hastie T. The Elements of Statistical Learning.\n",
    "* https://www.codecademy.com/learn/machine-learning\n",
    "* https://www.w3schools.com/python/python_ml_getting_started.asp\n",
    "* https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
